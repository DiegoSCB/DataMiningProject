{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okey here is explained what i have doned. I have used pandas for storing the results.\n",
    "\n",
    "The structure of the information: '.sqlite' files with one table named 'data' that contains all the tweets. The tweets has:\n",
    "    - Index, unique id of the tweet.\n",
    "    - Language, language of the tweet.\n",
    "    - Location, location of the user according to his profile information.\n",
    "    - List of the candidates (Arthaud, Asselineau, Cheminade, Dupont-Aignan, Filon, Hamon, Lassalle, Le-Pen, Macron, Melenchon, Poutou). The candidates have a 1 if the tweet is referred to them.\n",
    "    - Original Tweet, the original tweet if is a retweeted tweet or corresponds to someone answering the tweet.\n",
    "    - Original User, user that has written the original tweet.\n",
    "    - Retweeted, 1 or 0 depending if the tweets is retweeted or not.\n",
    "    - Tweet, text\n",
    "    - Timestamp in milliseconds of the hour that the tweet has been written\n",
    "    - User, name of the user\n",
    "    - Day\n",
    "    - This parameter i didnt know what it is but i have deleted it\n",
    "\n",
    "\n",
    "First i have read all the '.sqlites' file and i have filter to obtain only the ones that are written in English, that is the condition in the first if structure.\n",
    "\n",
    "Moreover, i have deleted some irrelevant data like the id of the tweet (because we have eliminate some of them), the language (in the English dataset) and the last paramater. I have pass the hour that was in miliseconds to normal datetime and renamed all the columns to have an idea of what structured our data is going to have.\n",
    "\n",
    "For the location, it has been used the package GeoText for cleaning the location and split it in two different fields Country and City. Someone of them are imposible to clean so this two new fields has used the location parameter to full fill them.\n",
    "\n",
    "I have used the vaderSentiment tool (you could install it via 'pip install vaderSentiment') and analyse the text to calculate the sentiment of each tweet. I have added this new column to the dataFrame and save the data in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import scandir, getcwd\n",
    "\n",
    "def ls(ruta = getcwd()+'/data/'):\n",
    "    return [arch.name for arch in scandir(ruta) if arch.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ls()\n",
    "data_en = list()\n",
    "all_data = list()\n",
    "for file in files:\n",
    "    con = lite.connect('data/'+file)\n",
    "\n",
    "    cur = con.cursor()    \n",
    "    cur.execute('SELECT * from data')\n",
    "\n",
    "    data = cur.fetchall()\n",
    "    for tweet in data:\n",
    "        data_en.append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geotext import GeoText\n",
    "\n",
    "def extract_city_country(location):\n",
    "    city = ''\n",
    "    country = ''\n",
    "    places = GeoText(location.lower().title().strip())\n",
    "    if len(places.cities) != 0:\n",
    "        city = places.cities[0]\n",
    "        if len(list(places.country_mentions.items())):\n",
    "            if places.cities[0] == \"Paris\":\n",
    "                country = \"FR\"\n",
    "            else: country = list(places.country_mentions.items())[0][0]\n",
    "    else:\n",
    "        if len(list(places.country_mentions.items())):\n",
    "            country = list(places.country_mentions.items())[0][0]\n",
    "    return city,country\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_jumplines(text):\n",
    "    if text != None:\n",
    "        text = re.sub('\\n',' ',text)\n",
    "        text = re.sub('\\r',' ',text)\n",
    "        text = re.sub('\\t',' ',text)\n",
    "        text = re.sub('\\'',' ',text)\n",
    "        text = re.sub('‘',' ',text)\n",
    "        text = re.sub('’',' ',text)\n",
    "        text = re.sub(';',' ',text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_final_en = list()\n",
    "\n",
    "for tweet in data_en:\n",
    "    if tweet[2]:\n",
    "        city,country = extract_city_country(tweet[2])\n",
    "        t = tweet + (city, country)\n",
    "    else:\n",
    "        t = tweet + (\"\",\"\")\n",
    "\n",
    "    data_final_en.append(t)\n",
    "data_en = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_tweets = list()\n",
    "for tweet in data_final_en:\n",
    "    if tweet[16] != '1':\n",
    "        filter_tweets.append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "jump = int(length/4)\n",
    "\n",
    "for i in range(4):\n",
    "    df = pandas.DataFrame(data=filter_tweets[(i)*jump:(i+1)*jump])\n",
    "    del df[0]\n",
    "    df[18] = df[18].apply(lambda x: time.strftime(\"%D %H:%M\", time.localtime(int(x)/1000)).split()[1])\n",
    "    del df[21] #delete\n",
    "    df.columns = ['Language','Location', 'Anthaud', 'Asselineau', 'Cheminade', 'Dupont-Aignan', 'Fillon', 'Hamon', 'Lassalle', 'Le-Pen', 'Macron',\n",
    "             'Melenchon', 'Poutou', 'Original-Tweet', 'Original-User', 'Retweeted', 'Tweet', 'Hour', 'User', 'Day', 'City', 'Country']\n",
    "    df['Tweet'] = df['Tweet'].apply(clean_jumplines)\n",
    "    df['Original-Tweet'] = df['Original-Tweet'].apply(clean_jumplines)\n",
    "    df[df.columns] = df[df.columns].replace({',':' '}, regex=True)\n",
    "    del df['Location']\n",
    "    df.to_csv('elections_tweets_nosentiment'+str(i)+'.csv',sep=',')\n",
    "    sentiment = list()\n",
    "\n",
    "    for tweet in df['Tweet']:\n",
    "            to_lang=\"en\"\n",
    "        from_lang=df['Language']\n",
    "        if (from_lang == \"en\") or (from_lang == \"en-US\"):\n",
    "            translation = sentence\n",
    "            translator_name = \"No translation needed\"\n",
    "        else: # please note usage limits for My Memory Translation Service:   http://mymemory.translated.net/doc/usagelimits.php\n",
    "            # using   MY MEMORY NET   http://mymemory.translated.net\n",
    "            api_url = \"http://mymemory.translated.net/api/get?q={}&langpair={}|{}\".format(sentence, from_lang, to_lang)\n",
    "            hdrs ={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                   'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                   'Accept-Encoding': 'none',\n",
    "                   'Accept-Language': 'en-US,en;q=0.8',\n",
    "                   'Connection': 'keep-alive'}\n",
    "        response = requests.get(api_url, headers=hdrs)\n",
    "        response_json = json.loads(response.text)\n",
    "        translation = response_json[\"responseData\"][\"translatedText\"]\n",
    "        translator_name = \"MemoryNet Translation Service\"\n",
    "        sentiment.append(analyzer.polarity_scores(transalation))\n",
    "    real_sentiments = list()\n",
    "    compound = list()\n",
    "    for s in sentiment:\n",
    "        if s['compound'] >= 0.5:\n",
    "            real_sentiments.append('Positive')\n",
    "        elif s['compound'] <= -0.5:\n",
    "            real_sentiments.append('Negative')\n",
    "        else:\n",
    "            real_sentiments.append('Neutral')\n",
    "        compound.append(s['compound'])\n",
    "    df['Sentiment'] = pandas.Series(real_sentiments)\n",
    "    df['Compound'] = pandas.Series(compound)\n",
    "    df.to_csv('elections_tweets'+str(i)+'.csv',sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
