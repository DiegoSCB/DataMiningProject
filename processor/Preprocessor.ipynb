{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okey here is explained what i have doned. I have used pandas for storing the results.\n",
    "\n",
    "The structure of the information: '.sqlite' files with one table named 'data' that contains all the tweets. The tweets has:\n",
    "    - Index, unique id of the tweet.\n",
    "    - Language, language of the tweet.\n",
    "    - Location, location of the user according to his profile information.\n",
    "    - List of the candidates (Arthaud, Asselineau, Cheminade, Dupont-Aignan, Filon, Hamon, Lassalle, Le-Pen, Macron, Melenchon, Poutou). The candidates have a 1 if the tweet is referred to them.\n",
    "    - Original Tweet, the original tweet if is a retweeted tweet or corresponds to someone answering the tweet.\n",
    "    - Original User, user that has written the original tweet.\n",
    "    - Retweeted, 1 or 0 depending if the tweets is retweeted or not.\n",
    "    - Tweet, text\n",
    "    - Timestamp in milliseconds of the hour that the tweet has been written\n",
    "    - User, name of the user\n",
    "    - Day\n",
    "    - This parameter i didnt know what it is but i have deleted it\n",
    "\n",
    "\n",
    "First i have read all the '.sqlites' file and i have filter to obtain only the ones that are written in English, that is the condition in the first if structure.\n",
    "\n",
    "Moreover i have deleted some irrelevant data like the id of the tweet (because we have eliminate some of them), the language and the last paramater. I have pass the hour that was in miliseconds to normal datetime and renamed all the columns to have an idea of what structured our data is going to have.\n",
    "\n",
    "I have used the vaderSentiment tool (you could install it via 'pip install vaderSentiment') and analyse the text to calculate the sentiment of each tweet. I have added this new column to the dataFrame and save the data in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import scandir, getcwd\n",
    "\n",
    "def ls(ruta = getcwd()+'/data/'):\n",
    "    return [arch.name for arch in scandir(ruta) if arch.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ls()\n",
    "data_en = list()\n",
    "all_data = list()\n",
    "for file in files:\n",
    "    con = lite.connect('data/'+file)\n",
    "\n",
    "    cur = con.cursor()    \n",
    "    cur.execute('SELECT * from data')\n",
    "\n",
    "    data = cur.fetchall()\n",
    "    for tweet in data:\n",
    "        if tweet[1] == 'en':\n",
    "            data_en.append(tweet)\n",
    "        #all_data.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geotext import GeoText\n",
    "\n",
    "def extract_city_country(location):\n",
    "    city = ''\n",
    "    country = ''\n",
    "    places = GeoText(location.lower().title().strip())\n",
    "    if len(places.cities) != 0:\n",
    "        city = places.cities[0]\n",
    "        if len(list(places.country_mentions.items())):\n",
    "            if places.cities[0] == \"Paris\":\n",
    "                country = \"FR\"\n",
    "            else: country = list(places.country_mentions.items())[0][0]\n",
    "    else:\n",
    "        if len(list(places.country_mentions.items())):\n",
    "            country = list(places.country_mentions.items())[0][0]\n",
    "    return city,country\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_final_en = list()\n",
    "\n",
    "for tweet in data_en:\n",
    "    if tweet[2]:\n",
    "        city,country = extract_city_country(tweet[2])\n",
    "        t = tweet + (city, country)\n",
    "    else:\n",
    "        t = tweet + (\"\",\"\")\n",
    "\n",
    "    data_final_en.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data=data_final_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df[0] #delete indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df[1] #delete language because we have extracted only English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# pass timestamp in millisecond to datetime\n",
    "df[18] = df[18].apply(lambda x: time.strftime(\"%D %H:%M\", time.localtime(int(x)/1000)).split()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df[21] #delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = ['Location', 'Anthaud', 'Asselineau', 'Cheminade', 'Dupont-Aignan', 'Fillon', 'Hamon', 'Lassalle', 'Le-Pen', 'Macron',\n",
    "             'Melenchon', 'Poutou', 'Original-Tweet', 'Original-User', 'Retweeted', 'Tweet', 'Hour', 'User', 'Day', 'City', 'Country']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_jumplines(text):\n",
    "    if text != None:\n",
    "        text = re.sub('\\n',' ',text)\n",
    "        text = re.sub('\\r',' ',text)\n",
    "        text = re.sub('\\t',' ',text)\n",
    "        text = re.sub('\\'',' ',text)\n",
    "        text = re.sub('‘',' ',text)\n",
    "        text = re.sub('’',' ',text)\n",
    "        text = re.sub(';',' ',text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(clean_jumplines)\n",
    "df['Original-Tweet'] = df['Original-Tweet'].apply(clean_jumplines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.columns] = df[df.columns].replace({',':' '}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df['Location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment = list()\n",
    "\n",
    "for tweet in df['Tweet']:\n",
    "    sentiment.append(analyzer.polarity_scores(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_sentiments = list()\n",
    "compound = list()\n",
    "for s in sentiment:\n",
    "    if s['compound'] >= 0.5:\n",
    "        real_sentiments.append('Positive')\n",
    "    elif s['compound'] <= -0.5:\n",
    "        real_sentiments.append('Negative')\n",
    "    else:\n",
    "        real_sentiments.append('Neutral')\n",
    "    compound.append(s['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Sentiment'] = pandas.Series(real_sentiments)\n",
    "df['Compound'] = pandas.Series(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = pandas.ExcelWriter('data-en.xlsx')\n",
    "df.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('elections_tweets_en.csv',sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
